{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tf_pegasus.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E3m_PK_UQKqi","outputId":"66fe5e6f-8c42-42c4-e432-0573bc4c48da","executionInfo":{"status":"ok","timestamp":1648454308365,"user_tz":-420,"elapsed":16509,"user":{"displayName":"Văn Nghiêm","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03593021723549294443"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install genz-tokenize -q"],"metadata":{"id":"s5tKNiBSIiDi"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q7XFh0af6iPP"},"outputs":[],"source":["import random\n","from typing import Optional, Tuple, Union, List\n","import numpy as np\n","import tensorflow as tf\n","from transformers import AutoModel, AutoTokenizer \n","import pandas as pd\n","from genz_tokenize import TokenizeForBert"]},{"cell_type":"code","source":["# model_name = \"google/pegasus-large\" \n","# tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"],"metadata":{"id":"H6irI3koRMxg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data=pd.read_csv('/content/drive/MyDrive/Huy/train_data.csv',encoding='utf-8')\n","ipt_data, opt_data = list(data['original']), list(data['summary'])"],"metadata":{"id":"oaDkUWR3gLQV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = TokenizeForBert()"],"metadata":{"id":"Ast6on6wQt3B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["LARGE_NEGATIVE = -1e8"],"metadata":{"id":"2HM5x-Md6z2-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_dataset=[]\n","for i in ipt_data:\n","  new_dataset.append(i)\n","for j in opt_data:\n","  new_dataset.append(j)"],"metadata":{"id":"IR6pTrx7p6WL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(new_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CVfCiKj5xG1y","outputId":"092ffeb1-9931-4f89-c9f8-4d8f3769f140","executionInfo":{"status":"ok","timestamp":1648454344009,"user_tz":-420,"elapsed":17,"user":{"displayName":"Văn Nghiêm","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03593021723549294443"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["210836"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# batch_size = 1024\n","# all_texts = [new_dataset[i : i + batch_size] for i in range(0, len(new_dataset), batch_size)]\n","# def batch_iterator():\n","#     for i in range(0, len(new_dataset), batch_size):\n","#         yield new_dataset[i : i + batch_size]\n","# new_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(), vocab_size=64000)"],"metadata":{"id":"sHKbGkoOtLl-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer.vocab_size"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C2LFAJX5vuHQ","outputId":"916e9525-55f2-4c62-bfd7-7b97794c2d62","executionInfo":{"status":"ok","timestamp":1648454344010,"user_tz":-420,"elapsed":14,"user":{"displayName":"Văn Nghiêm","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03593021723549294443"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["48423"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["import math\n","from packaging import version\n","\n","def _gelu(x):\n","    \"\"\"\n","    Gaussian Error Linear Unit. Original Implementation of the gelu activation function in Google Bert repo when\n","    initially created. For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n","    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3)))) Also see\n","    https://arxiv.org/abs/1606.08415\n","    \"\"\"\n","    x = tf.convert_to_tensor(x)\n","    cdf = 0.5 * (1.0 + tf.math.erf(x / tf.cast(tf.sqrt(2.0), x.dtype)))\n","\n","    return x * cdf\n","\n","\n","def _gelu_new(x):\n","    \"\"\"\n","    Gaussian Error Linear Unit. This is a smoother version of the GELU. Original paper: https://arxiv.org/abs/1606.0841\n","    Args:\n","        x: float Tensor to perform activation\n","    Returns:\n","        `x` with the GELU activation applied.\n","    \"\"\"\n","    x = tf.convert_to_tensor(x)\n","    pi = tf.cast(math.pi, x.dtype)\n","    coeff = tf.cast(0.044715, x.dtype)\n","    cdf = 0.5 * (1.0 + tf.tanh(tf.sqrt(2.0 / pi) * (x + coeff * tf.pow(x, 3))))\n","\n","    return x * cdf\n","\n","\n","def mish(x):\n","    x = tf.convert_to_tensor(x)\n","\n","    return x * tf.tanh(tf.math.softplus(x))\n","\n","\n","def gelu_fast(x):\n","    x = tf.convert_to_tensor(x)\n","    coeff1 = tf.cast(0.044715, x.dtype)\n","    coeff2 = tf.cast(0.7978845608, x.dtype)\n","\n","    return 0.5 * x * (1.0 + tf.tanh(x * coeff2 * (1.0 + coeff1 * x * x)))\n","\n","\n","def quick_gelu(x):\n","    x = tf.convert_to_tensor(x)\n","    coeff = tf.cast(1.702, x.dtype)\n","    return x * tf.math.sigmoid(coeff * x)\n","\n","\n","def gelu_10(x):\n","    return tf.clip_by_value(_gelu(x), -10, 10)\n","\n","\n","def glu(x, axis=-1):\n","    a, b = tf.split(x, 2, axis=axis)\n","    return a * tf.math.sigmoid(b)\n","\n","\n","if version.parse(tf.version.VERSION) >= version.parse(\"2.4\"):\n","\n","    def approximate_gelu_wrap(x):\n","        return tf.keras.activations.gelu(x, approximate=True)\n","\n","    gelu = tf.keras.activations.gelu\n","    gelu_new = approximate_gelu_wrap\n","else:\n","    gelu = _gelu\n","    gelu_new = _gelu_new\n","\n","\n","ACT2FN = {\n","    \"gelu\": gelu,\n","    \"relu\": tf.keras.activations.relu,\n","    \"swish\": tf.keras.activations.swish,\n","    \"silu\": tf.keras.activations.swish,\n","    \"gelu_new\": gelu_new,\n","    \"mish\": mish,\n","    \"tanh\": tf.keras.activations.tanh,\n","    \"gelu_fast\": gelu_fast,\n","    \"quick_gelu\": quick_gelu,\n","    \"gelu_10\": gelu_10,\n","    \"glu\": glu,\n","}\n","\n","\n","def get_tf_activation(activation_string):\n","    if activation_string in ACT2FN:\n","        return ACT2FN[activation_string]\n","    else:\n","        raise KeyError(f\"function {activation_string} not found in ACT2FN mapping {list(ACT2FN.keys())}\")"],"metadata":{"id":"FVtfXXjWjbHQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PegasusConfig():\n","    \n","    def __init__(\n","        self,\n","        vocab_size=tokenizer.vocab_size,\n","        max_position_embeddings=1026,\n","        encoder_layers=6,\n","        encoder_ffn_dim=2048,\n","        encoder_attention_heads=8,\n","        decoder_layers=6,\n","        decoder_ffn_dim=2048,\n","        decoder_attention_heads=8,\n","        encoder_layerdrop=0.0,\n","        decoder_layerdrop=0.0,\n","        use_cache=True,\n","        is_encoder_decoder=True,\n","        activation_function=\"gelu\",\n","        d_model=512,\n","        dropout=0.1,\n","        attention_dropout=0.0,\n","        activation_dropout=0.0,\n","        init_std=0.02,\n","        decoder_start_token_id=0,\n","        classifier_dropout=0.0,\n","        scale_embedding=False,\n","        pad_token_id=0,\n","        eos_token_id=1,\n","        forced_eos_token_id=1,\n","        **kwargs\n","    ):\n","        self.vocab_size = vocab_size\n","        self.max_position_embeddings = max_position_embeddings\n","        self.d_model = d_model\n","        self.hidden_size = d_model\n","        self.encoder_ffn_dim = encoder_ffn_dim\n","        self.encoder_layers = encoder_layers\n","        self.encoder_attention_heads = encoder_attention_heads\n","        self.decoder_ffn_dim = decoder_ffn_dim\n","        self.decoder_layers = decoder_layers\n","        self.decoder_attention_heads = decoder_attention_heads\n","        self.dropout = dropout\n","        self.attention_dropout = attention_dropout\n","        self.activation_dropout = activation_dropout\n","        self.activation_function = activation_function\n","        self.init_std = init_std\n","        self.encoder_layerdrop = encoder_layerdrop\n","        self.decoder_layerdrop = decoder_layerdrop\n","        self.classifier_dropout = classifier_dropout\n","        self.use_cache = use_cache\n","        self.num_hidden_layers = encoder_layers\n","        self.scale_embedding = scale_embedding\n","        self.pad_token_id = pad_token_id\n","        self.output_hidden_states = True "],"metadata":{"id":"k1_kgQgUQQ8E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["config = PegasusConfig()\n","config.d_model = 256\n","config.encoder_ffn_dim = 1024\n","config.decoder_ffn_dim = 1024"],"metadata":{"id":"K602YUvcQh7c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_initializer(initializer_range: float = 0.02) -> tf.initializers.TruncatedNormal:\n","    return tf.keras.initializers.TruncatedNormal(stddev=initializer_range)\n","    \n","def shape_list(tensor: Union[tf.Tensor, np.ndarray]) -> List[int]:\n","    \n","    if isinstance(tensor, np.ndarray):\n","        return list(tensor.shape)\n","\n","    dynamic = tf.shape(tensor)\n","\n","    if tensor.shape == tf.TensorShape(None):\n","        return dynamic\n","\n","    static = tensor.shape.as_list()\n","\n","    return [dynamic[i] if s is None else s for i, s in enumerate(static)]"],"metadata":{"id":"9ijt9GqThuNn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TFSharedEmbeddings(tf.keras.layers.Layer):\n","\n","    def __init__(self, vocab_size: int, hidden_size: int, initializer_range: Optional[float] = None, **kwargs):\n","        super().__init__(**kwargs)\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","        self.initializer_range = hidden_size**-0.5 if initializer_range is None else initializer_range\n","\n","    def build(self, input_shape):\n","\n","        self.weight = self.add_weight(\n","            \"weight\", shape=[self.vocab_size, self.hidden_size], initializer=get_initializer(self.initializer_range)\n","        )\n","        super().build(input_shape)\n","\n","    def get_config(self):\n","        config = {\n","            \"vocab_size\": self.vocab_size,\n","            \"hidden_size\": self.hidden_size,\n","            \"initializer_range\": self.initializer_range,\n","        }\n","        base_config = super().get_config()\n","\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","    def call(self, inputs: tf.Tensor, mode: str = \"embedding\") -> tf.Tensor:\n","        if mode == \"embedding\":\n","            return self._embedding(inputs)\n","        elif mode == \"linear\":\n","            return self._linear(inputs)\n","        else:\n","            raise ValueError(f\"mode {mode} is not valid.\")\n","\n","    def _embedding(self, input_ids):\n","        \"\"\"Applies embedding based on inputs tensor.\"\"\"\n","        return tf.gather(self.weight, input_ids)\n","\n","    def _linear(self, inputs):\n","        \"\"\"\n","        Computes logits by running inputs through a linear layer.\n","        Args:\n","            inputs: A float32 tensor with shape [..., hidden_size]\n","        Returns:\n","            float32 tensor with shape [..., vocab_size].\n","        \"\"\"\n","        first_dims = shape_list(inputs)[:-1]\n","        x = tf.reshape(inputs, [-1, self.hidden_size])\n","        logits = tf.matmul(x, self.weight, transpose_b=True)\n","\n","        return tf.reshape(logits, first_dims + [self.vocab_size])"],"metadata":{"id":"6j5Ir3eqDuU0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TFWrappedEmbeddings():\n","\n","    def __init__(self, layer, abs_scope_name=None):\n","        self._layer = layer\n","        self._abs_scope_name = abs_scope_name\n","\n","    def call(self, inputs, mode=\"embedding\"):\n","        if self._abs_scope_name is None:\n","            return self._layer.call(inputs, mode)\n","\n","        # if an abs scope name is given to the embedding variable, call variable from absolute scope\n","        with tf.compat.v1.variable_scope(self._abs_scope_name, auxiliary_name_scope=False) as abs_scope_name:\n","            with tf.name_scope(abs_scope_name.original_name_scope):\n","                return self._layer.call(inputs, mode)\n","\n","    def __call__(self, inputs, mode=\"embedding\"):\n","        if self._abs_scope_name is None:\n","            return self._layer(inputs, mode)\n","\n","        # if an abs scope name is given to the embedding variable, call variable from absolute scope\n","        with tf.compat.v1.variable_scope(self._abs_scope_name, auxiliary_name_scope=False) as abs_scope_name:\n","            with tf.name_scope(abs_scope_name.original_name_scope):\n","                return self._layer(inputs, mode)"],"metadata":{"id":"x5IGuRg5Ds1W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _make_causal_mask(input_ids_shape: tf.TensorShape, past_key_values_length: int = 0):\n","    \"\"\"\n","    Make causal mask used for bi-directional self-attention.\n","    \"\"\"\n","    bsz, tgt_len = input_ids_shape\n","    mask = tf.ones((tgt_len, tgt_len)) * LARGE_NEGATIVE\n","    mask_cond = tf.range(shape_list(mask)[-1])\n","\n","    mask = tf.where(mask_cond < tf.reshape(mask_cond + 1, (shape_list(mask)[-1], 1)), 0.0, mask)\n","\n","    if past_key_values_length > 0:\n","        mask = tf.concat([tf.zeros((tgt_len, past_key_values_length)), mask], axis=-1)\n","\n","    return tf.tile(mask[None, None, :, :], (bsz, 1, 1, 1))"],"metadata":{"id":"4HAJYVK46oNj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _expand_mask(mask: tf.Tensor, tgt_len: Optional[int] = None, past_key_values_length: int = 0):\n","    \"\"\"\n","    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n","    \"\"\"\n","    src_len = shape_list(mask)[1]\n","    tgt_len = tgt_len if tgt_len is not None else src_len\n","    one_cst = tf.constant(1.0)\n","    mask = tf.cast(mask, dtype=one_cst.dtype)\n","    expanded_mask = tf.tile(mask[:, None, None, :], (1, 1, tgt_len, 1))\n","\n","    return (one_cst - expanded_mask) * LARGE_NEGATIVE"],"metadata":{"id":"qZJCqATa6qfO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TFPegasusSinusoidalPositionalEmbedding(tf.keras.layers.Layer):\n","    \"\"\"This module produces sinusoidal positional embeddings of any length.\"\"\"\n","\n","    def __init__(self, num_positions: int, embedding_dim: int, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        if embedding_dim % 2 != 0:\n","            raise NotImplementedError(f\"odd embedding_dim {embedding_dim} not supported\")\n","\n","        self.embedding_dim = embedding_dim\n","        self.num_positions = num_positions\n","\n","    def build(self, input_shape: tf.TensorShape):\n","\n","        weight = self._init_weight(self.num_positions, self.embedding_dim)\n","\n","        self.weight = self.add_weight(\n","            name=\"embeddings\",\n","            shape=[self.num_positions, self.embedding_dim],\n","        )\n","        weight = tf.cast(weight, dtype=self.weight.dtype)\n","\n","        self.weight.assign(weight)\n","\n","        super().build(input_shape)\n","\n","    @staticmethod\n","    def _init_weight(n_pos: int, dim: int):\n","        \"\"\"\n","        Identical to the XLM create_sinusoidal_embeddings except features are not interleaved. The cos features are in\n","        the 2nd half of the vector. [dim // 2:]\n","        \"\"\"\n","        position_enc = np.array(\n","            [[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)]\n","        )\n","        table = np.zeros_like(position_enc)\n","        # index 0 is all zero\n","        table[:, 0 : dim // 2] = np.sin(position_enc[:, 0::2])\n","        table[:, dim // 2 :] = np.cos(position_enc[:, 1::2])\n","        # convert to tensor\n","        table = tf.convert_to_tensor(table)\n","        tf.stop_gradient(table)\n","        return table\n","\n","    def call(self, input_shape: tf.TensorShape, past_key_values_length: int = 0):\n","        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n","        bsz, seq_len = input_shape[:2]\n","\n","        positions = tf.range(past_key_values_length, seq_len + past_key_values_length, delta=1, name=\"range\")\n","        return tf.gather(self.weight, positions)"],"metadata":{"id":"4khW-oTq7Ftb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TFPegasusAttention(tf.keras.layers.Layer):\n","    \"\"\"Multi-headed attention from \"Attention Is All You Need\"\"\"\n","\n","    def __init__(\n","        self,\n","        embed_dim: int,\n","        num_heads: int,\n","        dropout: float = 0.0,\n","        is_decoder: bool = False,\n","        bias: bool = True,\n","        **kwargs,\n","    ):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","\n","        self.num_heads = num_heads\n","        self.dropout = tf.keras.layers.Dropout(dropout)\n","        self.head_dim = embed_dim // num_heads\n","        if (self.head_dim * num_heads) != self.embed_dim:\n","            raise ValueError(\n","                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}\"\n","                f\" and `num_heads`: {num_heads}).\"\n","            )\n","        self.scaling = self.head_dim**-0.5\n","        self.is_decoder = is_decoder\n","\n","        self.k_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"k_proj\")\n","        self.q_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"q_proj\")\n","        self.v_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"v_proj\")\n","        self.out_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"out_proj\")\n","\n","    def _shape(self, tensor: tf.Tensor, seq_len: int, bsz: int):\n","        return tf.transpose(tf.reshape(tensor, (bsz, seq_len, self.num_heads, self.head_dim)), (0, 2, 1, 3))\n","\n","    def call(\n","        self,\n","        hidden_states: tf.Tensor,\n","        key_value_states: Optional[tf.Tensor] = None,\n","        past_key_value: Optional[Tuple[Tuple[tf.Tensor]]] = None,\n","        attention_mask: Optional[tf.Tensor] = None,\n","        layer_head_mask: Optional[tf.Tensor] = None,\n","        training: Optional[bool] = False,\n","    ) -> Tuple[tf.Tensor, Optional[tf.Tensor]]:\n","        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n","\n","        # if key_value_states are provided this layer is used as a cross-attention layer\n","        # for the decoder\n","        is_cross_attention = key_value_states is not None\n","        bsz, tgt_len, embed_dim = shape_list(hidden_states)\n","\n","        # get query proj\n","        query_states = self.q_proj(hidden_states) * self.scaling\n","        # get key, value proj\n","        if is_cross_attention and past_key_value is not None:\n","            # reuse k,v, cross_attentions\n","            key_states = past_key_value[0]\n","            value_states = past_key_value[1]\n","        elif is_cross_attention:\n","            # cross_attentions\n","            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n","            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n","        elif past_key_value is not None:\n","            # reuse k, v, self_attention\n","            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n","            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n","            key_states = tf.concat([past_key_value[0], key_states], axis=2)\n","            value_states = tf.concat([past_key_value[1], value_states], axis=2)\n","        else:\n","            # self_attention\n","            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n","            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n","\n","        if self.is_decoder:\n","            # if cross_attention save Tuple(tf.Tensor, tf.Tensor) of all cross attention key/value_states.\n","            # Further calls to cross_attention layer can then reuse all cross-attention\n","            # key/value_states (first \"if\" case)\n","            # if uni-directional self-attention (decoder) save Tuple(tf.Tensor, tf.Tensor) of\n","            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n","            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n","            # if encoder bi-directional self-attention `past_key_value` is always `None`\n","            past_key_value = (key_states, value_states)\n","\n","        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n","        query_states = tf.reshape(self._shape(query_states, tgt_len, bsz), proj_shape)\n","        key_states = tf.reshape(key_states, proj_shape)\n","        value_states = tf.reshape(value_states, proj_shape)\n","\n","        src_len = shape_list(key_states)[1]\n","        attn_weights = tf.matmul(query_states, key_states, transpose_b=True)\n","\n","        # The tf.debugging asserts are not compliant with XLA then they\n","        # have to be disabled in other modes than eager.\n","        if tf.executing_eagerly():\n","            tf.debugging.assert_equal(\n","                shape_list(attn_weights),\n","                [bsz * self.num_heads, tgt_len, src_len],\n","                message=f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is {shape_list(attn_weights)}\",\n","            )\n","\n","        if attention_mask is not None:\n","            # The tf.debugging asserts are not compliant with XLA then they\n","            # have to be disabled in other modes than eager.\n","            if tf.executing_eagerly():\n","                tf.debugging.assert_equal(\n","                    shape_list(attention_mask),\n","                    [bsz, 1, tgt_len, src_len],\n","                    message=f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {shape_list(attention_mask)}\",\n","                )\n","\n","            attention_mask = tf.cast(attention_mask, dtype=attn_weights.dtype)\n","            attn_weights = tf.reshape(attn_weights, (bsz, self.num_heads, tgt_len, src_len)) + attention_mask\n","            attn_weights = tf.reshape(attn_weights, (bsz * self.num_heads, tgt_len, src_len))\n","\n","        attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n","\n","        if layer_head_mask is not None:\n","            # The tf.debugging asserts are not compliant with XLA then they\n","            # have to be disabled in other modes than eager.\n","            if tf.executing_eagerly():\n","                tf.debugging.assert_equal(\n","                    shape_list(layer_head_mask),\n","                    [self.num_heads],\n","                    message=f\"Head mask for a single layer should be of size {(self.num_heads)}, but is {shape_list(layer_head_mask)}\",\n","                )\n","\n","            attn_weights = tf.reshape(layer_head_mask, (1, -1, 1, 1)) * tf.reshape(\n","                attn_weights, (bsz, self.num_heads, tgt_len, src_len)\n","            )\n","            attn_weights = tf.reshape(attn_weights, (bsz * self.num_heads, tgt_len, src_len))\n","\n","        attn_probs = self.dropout(attn_weights, training=training)\n","        attn_output = tf.matmul(attn_probs, value_states)\n","\n","        # The tf.debugging asserts are not compliant with XLA then they\n","        # have to be disabled in other modes than eager.\n","        if tf.executing_eagerly():\n","            tf.debugging.assert_equal(\n","                shape_list(attn_output),\n","                [bsz * self.num_heads, tgt_len, self.head_dim],\n","                message=f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is {shape_list(attn_output)}\",\n","            )\n","\n","        attn_output = tf.transpose(\n","            tf.reshape(attn_output, (bsz, self.num_heads, tgt_len, self.head_dim)), (0, 2, 1, 3)\n","        )\n","        attn_output = tf.reshape(attn_output, (bsz, tgt_len, embed_dim))\n","\n","        attn_output = self.out_proj(attn_output)\n","        attn_weights: tf.Tensor = tf.reshape(attn_weights, (bsz, self.num_heads, tgt_len, src_len))\n","\n","        return attn_output, attn_weights, past_key_value"],"metadata":{"id":"0zxkrDOe7OSU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TFPegasusEncoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = config.d_model\n","        self.self_attn = TFPegasusAttention(\n","            self.embed_dim, config.encoder_attention_heads, dropout=config.attention_dropout, name=\"self_attn\"\n","        )\n","        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n","        self.dropout = tf.keras.layers.Dropout(config.dropout)\n","        self.activation_fn = get_tf_activation(config.activation_function)\n","        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n","        self.fc1 = tf.keras.layers.Dense(config.encoder_ffn_dim, name=\"fc1\")\n","        self.fc2 = tf.keras.layers.Dense(self.embed_dim, name=\"fc2\")\n","        self.final_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"final_layer_norm\")\n","\n","    def call(\n","        self,\n","        hidden_states: tf.Tensor,\n","        attention_mask: tf.Tensor,\n","        layer_head_mask: tf.Tensor,\n","        training: Optional[bool] = False,\n","    ):\n","        \"\"\"\n","        Args:\n","            hidden_states (`tf.Tensor`): input to the layer of shape *(seq_len, batch, embed_dim)*\n","            attention_mask (`tf.Tensor`): attention mask of size\n","                *(batch, 1, tgt_len, src_len)* where padding elements are indicated by very large negative values.\n","            layer_head_mask (`tf.Tensor`): mask for attention heads in a given layer of size\n","                *(encoder_attention_heads,)*\n","        \"\"\"\n","        residual = hidden_states\n","        hidden_states = self.self_attn_layer_norm(hidden_states)\n","        hidden_states, self_attn_weights, _ = self.self_attn(\n","            hidden_states=hidden_states, attention_mask=attention_mask, layer_head_mask=layer_head_mask\n","        )\n","\n","        # The tf.debugging asserts are not compliant with XLA then they\n","        # have to be disabled in other modes than eager.\n","        if tf.executing_eagerly():\n","            tf.debugging.assert_equal(\n","                shape_list(hidden_states),\n","                shape_list(residual),\n","                message=f\"Self attn modified the shape of query {shape_list(residual)} to {shape_list(hidden_states)}\",\n","            )\n","\n","        hidden_states = self.dropout(hidden_states, training=training)\n","        hidden_states = residual + hidden_states\n","\n","        residual = hidden_states\n","        hidden_states = self.final_layer_norm(hidden_states)\n","        hidden_states = self.activation_fn(self.fc1(hidden_states))\n","        hidden_states = self.activation_dropout(hidden_states, training=training)\n","        hidden_states = self.fc2(hidden_states)\n","        hidden_states = self.dropout(hidden_states, training=training)\n","        hidden_states = residual + hidden_states\n","\n","        return hidden_states, self_attn_weights"],"metadata":{"id":"Q5kcxlB_77kg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TFPegasusDecoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = config.d_model\n","        self.self_attn = TFPegasusAttention(\n","            embed_dim=self.embed_dim,\n","            num_heads=config.decoder_attention_heads,\n","            dropout=config.attention_dropout,\n","            name=\"self_attn\",\n","            is_decoder=True,\n","        )\n","        self.dropout = tf.keras.layers.Dropout(config.dropout)\n","        self.activation_fn = get_tf_activation(config.activation_function)\n","        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n","\n","        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n","        self.encoder_attn = TFPegasusAttention(\n","            self.embed_dim,\n","            config.decoder_attention_heads,\n","            dropout=config.attention_dropout,\n","            name=\"encoder_attn\",\n","            is_decoder=True,\n","        )\n","        self.encoder_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"encoder_attn_layer_norm\")\n","        self.fc1 = tf.keras.layers.Dense(config.decoder_ffn_dim, name=\"fc1\")\n","        self.fc2 = tf.keras.layers.Dense(self.embed_dim, name=\"fc2\")\n","        self.final_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"final_layer_norm\")\n","\n","    def call(\n","        self,\n","        hidden_states: tf.Tensor,\n","        attention_mask: Optional[tf.Tensor] = None,\n","        encoder_hidden_states: Optional[tf.Tensor] = None,\n","        encoder_attention_mask: Optional[tf.Tensor] = None,\n","        layer_head_mask: Optional[tf.Tensor] = None,\n","        cross_attn_layer_head_mask: Optional[tf.Tensor] = None,\n","        past_key_value: Optional[Tuple[tf.Tensor]] = None,\n","        training: Optional[bool] = False,\n","    ) -> Tuple[tf.Tensor, tf.Tensor, Tuple[Tuple[tf.Tensor]]]:\n","        \"\"\"\n","        Args:\n","            hidden_states (`tf.Tensor`): input to the layer of shape *(seq_len, batch, embed_dim)*\n","            attention_mask (`tf.Tensor`): attention mask of size\n","                *(batch, 1, tgt_len, src_len)* where padding elements are indicated by very large negative values.\n","            encoder_hidden_states (`tf.Tensor`):\n","                cross attention input to the layer of shape *(seq_len, batch, embed_dim)*\n","            encoder_attention_mask (`tf.Tensor`): encoder attention mask of size\n","                *(batch, 1, tgt_len, src_len)* where padding elements are indicated by very large negative values.\n","            layer_head_mask (`tf.Tensor`): mask for attention heads in a given layer of size\n","                *(decoder_attention_heads,)*\n","            cross_attn_layer_head_mask (`tf.Tensor`): mask for heads of the cross-attention module.\n","                *(decoder_attention_heads,)*\n","            past_key_value (`Tuple(tf.Tensor)`): cached past key and value projection states\n","        \"\"\"\n","        residual = hidden_states\n","        hidden_states = self.self_attn_layer_norm(hidden_states)\n","\n","        # Self Attention\n","        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n","        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n","        # add present self-attn cache to positions 1,2 of present_key_value tuple\n","        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n","            hidden_states=hidden_states,\n","            past_key_value=self_attn_past_key_value,\n","            attention_mask=attention_mask,\n","            layer_head_mask=layer_head_mask,\n","        )\n","        hidden_states = self.dropout(hidden_states, training=training)\n","        hidden_states = residual + hidden_states\n","\n","        # Cross-Attention Block\n","        cross_attn_present_key_value = None\n","        cross_attn_weights = None\n","        if encoder_hidden_states is not None:\n","            residual = hidden_states\n","            hidden_states = self.encoder_attn_layer_norm(hidden_states)\n","\n","            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n","            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n","            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n","                hidden_states=hidden_states,\n","                key_value_states=encoder_hidden_states,\n","                attention_mask=encoder_attention_mask,\n","                layer_head_mask=cross_attn_layer_head_mask,\n","                past_key_value=cross_attn_past_key_value,\n","            )\n","            hidden_states = self.dropout(hidden_states, training=training)\n","            hidden_states = residual + hidden_states\n","\n","            # add cross-attn to positions 3,4 of present_key_value tuple\n","            present_key_value = present_key_value + cross_attn_present_key_value\n","\n","        # Fully Connected\n","        residual = hidden_states\n","        hidden_states = self.final_layer_norm(hidden_states)\n","        hidden_states = self.activation_fn(self.fc1(hidden_states))\n","        hidden_states = self.activation_dropout(hidden_states, training=training)\n","        hidden_states = self.fc2(hidden_states)\n","        hidden_states = self.dropout(hidden_states, training=training)\n","        hidden_states = residual + hidden_states\n","\n","        return (\n","            hidden_states,\n","            self_attn_weights,\n","            cross_attn_weights,\n","            present_key_value,\n","        )"],"metadata":{"id":"2VR0Wysx78MT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TFPegasusEncoder(tf.keras.layers.Layer):\n","    def __init__(self, config, embed_tokens, **kwargs):\n","        super().__init__(**kwargs)\n","        self.config = config\n","        self.dropout = tf.keras.layers.Dropout(config.dropout)\n","        self.layerdrop = config.encoder_layerdrop\n","        self.padding_idx = config.pad_token_id\n","        self.max_source_positions = config.max_position_embeddings\n","        self.embed_scale = tf.math.sqrt(float(config.d_model)) if config.scale_embedding else 1.0\n","\n","        self.embed_tokens = embed_tokens\n","        self.embed_positions = TFPegasusSinusoidalPositionalEmbedding(\n","            config.max_position_embeddings,\n","            config.d_model,\n","            name=\"embed_positions\",\n","        )\n","        self.layers = [TFPegasusEncoderLayer(config, name=f\"layers.{i}\") for i in range(config.encoder_layers)]\n","        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"layer_norm\")\n","\n","    def get_embed_tokens(self):\n","        return self.embed_tokens\n","\n","    def set_embed_tokens(self, embed_tokens):\n","        self.embed_tokens = embed_tokens\n","\n","    def call(\n","        self,\n","        input_ids=None,\n","        inputs_embeds=None,\n","        attention_mask=None,\n","        head_mask=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","        training=False,\n","        **kwargs,\n","    ):\n","        if input_ids is not None and inputs_embeds is not None:\n","            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n","        elif input_ids is not None:\n","            input_shape = shape_list(input_ids)\n","        elif inputs_embeds is not None:\n","            input_shape = shape_list(inputs_embeds)[:-1]\n","        else:\n","            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n","\n","        if inputs_embeds is None:\n","            inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n","\n","        embed_pos = self.embed_positions(input_shape)\n","        hidden_states = inputs_embeds + embed_pos\n","        hidden_states = self.dropout(hidden_states, training=training)\n","\n","        # check attention mask and invert\n","        if attention_mask is not None:\n","            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n","            attention_mask = _expand_mask(attention_mask)\n","        else:\n","            attention_mask = None\n","\n","        encoder_states = () if output_hidden_states else None\n","        all_attentions = () if output_attentions else None\n","\n","        # check if head_mask has a correct number of layers specified if desired\n","        # The tf.debugging asserts are not compliant with XLA then they\n","        # have to be disabled in other modes than eager.\n","        if head_mask is not None and tf.executing_eagerly():\n","            tf.debugging.assert_equal(\n","                shape_list(head_mask)[0],\n","                len(self.layers),\n","                message=f\"The head_mask should be specified for {len(self.layers)} layers, but it is for {shape_list(head_mask)[0]}.\",\n","            )\n","\n","        # encoder layers\n","        for idx, encoder_layer in enumerate(self.layers):\n","\n","            if output_hidden_states:\n","                encoder_states = encoder_states + (hidden_states,)\n","            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n","            dropout_probability = random.uniform(0, 1)\n","            if training and (dropout_probability < self.layerdrop):  # skip the layer\n","                continue\n","\n","            hidden_states, attn = encoder_layer(\n","                hidden_states,\n","                attention_mask,\n","                head_mask[idx] if head_mask is not None else None,\n","            )\n","\n","            if output_attentions:\n","                all_attentions += (attn,)\n","\n","        hidden_states = self.layer_norm(hidden_states)\n","\n","        if output_hidden_states:\n","            encoder_states = encoder_states + (hidden_states,)\n","\n","        if not return_dict:\n","            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n","        last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n","        return (\n","            last_hidden_state, hidden_states, attentions\n","        )"],"metadata":{"id":"pvv17_o48DY8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TFPegasusDecoder(tf.keras.layers.Layer):\n","    \"\"\"\n","    Transformer decoder consisting of *config.decoder_layers* layers. Each layer is a [`TFPegasusDecoderLayer`]\n","    Args:\n","        config: PegasusConfig\n","        embed_tokens: output embedding\n","    \"\"\"\n","    def __init__(self, config, embed_tokens, **kwargs):\n","        super().__init__(**kwargs)\n","        self.config = config\n","        self.padding_idx = config.pad_token_id\n","        self.embed_tokens = embed_tokens\n","        self.layerdrop = config.decoder_layerdrop\n","        self.embed_positions = TFPegasusSinusoidalPositionalEmbedding(\n","            config.max_position_embeddings,\n","            config.d_model,\n","            name=\"embed_positions\",\n","        )\n","        self.embed_scale = tf.math.sqrt(float(config.d_model)) if config.scale_embedding else 1.0\n","        self.layers = [TFPegasusDecoderLayer(config, name=f\"layers.{i}\") for i in range(config.decoder_layers)]\n","        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"layer_norm\")\n","\n","        self.dropout = tf.keras.layers.Dropout(config.dropout)\n","\n","    def get_embed_tokens(self):\n","        return self.embed_tokens\n","\n","    def set_embed_tokens(self, embed_tokens):\n","        self.embed_tokens = embed_tokens\n","\n","    def call(\n","        self,\n","        input_ids=None,\n","        inputs_embeds=None,\n","        attention_mask=None,\n","        encoder_hidden_states=None,\n","        encoder_attention_mask=None,\n","        head_mask=None,\n","        cross_attn_head_mask=None,\n","        past_key_values=None,\n","        use_cache=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","        training=False,\n","        **kwargs,\n","    ):\n","\n","        if input_ids is not None and inputs_embeds is not None:\n","            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n","        elif input_ids is not None:\n","            input_shape = shape_list(input_ids)\n","        elif inputs_embeds is not None:\n","            input_shape = shape_list(inputs_embeds)[:-1]\n","        else:\n","            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n","\n","        past_key_values_length = shape_list(past_key_values[0][0])[2] if past_key_values is not None else 0\n","\n","        # embed positions\n","        positions = self.embed_positions(input_shape, past_key_values_length)\n","\n","        if inputs_embeds is None:\n","            inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n","\n","        hidden_states = inputs_embeds\n","\n","        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n","        if input_shape[-1] > 1:\n","            combined_attention_mask = _make_causal_mask(input_shape, past_key_values_length=past_key_values_length)\n","        else:\n","            combined_attention_mask = _expand_mask(\n","                tf.ones((input_shape[0], input_shape[1] + past_key_values_length)), tgt_len=input_shape[-1]\n","            )\n","\n","        if attention_mask is not None:\n","            combined_attention_mask = combined_attention_mask + _expand_mask(attention_mask, tgt_len=input_shape[-1])\n","\n","        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n","            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n","            encoder_attention_mask = _expand_mask(encoder_attention_mask, tgt_len=input_shape[-1])\n","\n","        hidden_states = self.dropout(hidden_states + positions, training=training)\n","\n","        # decoder layers\n","        all_hidden_states = () if output_hidden_states else None\n","        all_self_attns = () if output_attentions else None\n","        all_cross_attns = () if (output_attentions and encoder_hidden_states is not None) else None\n","        present_key_values = () if use_cache else None\n","\n","        # check if head_mask and cross_attn_head_mask have a correct number of layers specified if desired\n","        # The tf.debugging asserts are not compliant with XLA then they\n","        # have to be disabled in other modes than eager.\n","        for attn_mask_name, attn_mask in [(\"head_mask\", head_mask), (\"cross_attn_head_mask\", cross_attn_head_mask)]:\n","            if attn_mask is not None and tf.executing_eagerly():\n","                tf.debugging.assert_equal(\n","                    shape_list(attn_mask)[0],\n","                    len(self.layers),\n","                    message=f\"The {attn_mask_name} should be specified for {len(self.layers)} layers, but it is for {shape_list(attn_mask)[0]}.\",\n","                )\n","\n","        for idx, decoder_layer in enumerate(self.layers):\n","            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n","            if output_hidden_states:\n","                all_hidden_states += (hidden_states,)\n","            dropout_probability = random.uniform(0, 1)\n","\n","            if training and (dropout_probability < self.layerdrop):\n","                continue\n","\n","            past_key_value = past_key_values[idx] if past_key_values is not None else None\n","\n","            hidden_states, layer_self_attn, layer_cross_attn, present_key_value = decoder_layer(\n","                hidden_states,\n","                attention_mask=combined_attention_mask,\n","                encoder_hidden_states=encoder_hidden_states,\n","                encoder_attention_mask=encoder_attention_mask,\n","                layer_head_mask=head_mask[idx] if head_mask is not None else None,\n","                cross_attn_layer_head_mask=cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n","                past_key_value=past_key_value,\n","            )\n","\n","            if use_cache:\n","                present_key_values += (present_key_value,)\n","\n","            if output_attentions:\n","                all_self_attns += (layer_self_attn,)\n","\n","                if encoder_hidden_states is not None:\n","                    all_cross_attns += (layer_cross_attn,)\n","\n","        hidden_states = self.layer_norm(hidden_states)\n","\n","        if output_hidden_states:\n","            all_hidden_states += (hidden_states,)\n","\n","        if not return_dict:\n","            return hidden_states, present_key_values, all_hidden_states, all_self_attns, all_cross_attns\n","        else:\n","            last_hidden_state=hidden_states\n","            past_key_values=present_key_values\n","            hidden_states=all_hidden_states\n","            attentions=all_self_attns\n","            cross_attentions=all_cross_attns\n","            return (\n","                last_hidden_state,\n","                past_key_values,\n","                hidden_states,\n","                attentions,\n","                cross_attentions,\n","            )"],"metadata":{"id":"0MxXTJ1RBF35"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TFPegasusMainLayer(tf.keras.layers.Layer):\n","\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.config = config\n","        self.shared = TFSharedEmbeddings(config.vocab_size, config.d_model, config.pad_token_id, name=\"model.shared\")\n","\n","        with tf.compat.v1.variable_scope(\"model.shared\") as shared_abs_scope_name:\n","            pass\n","\n","        # Wraps layer to avoid problems with weight restoring and ensuring we're in the correct TF scope.\n","        embed_tokens = TFWrappedEmbeddings(self.shared, abs_scope_name=shared_abs_scope_name)\n","        embed_tokens.vocab_size = self.shared.vocab_size\n","        embed_tokens.hidden_size = self.shared.hidden_size\n","\n","        self.encoder = TFPegasusEncoder(config, embed_tokens, name=\"encoder\")\n","        self.decoder = TFPegasusDecoder(config, embed_tokens, name=\"decoder\")\n","\n","    def get_input_embeddings(self):\n","        return self.shared\n","\n","    def set_input_embeddings(self, new_embeddings):\n","        self.shared.weight = new_embeddings\n","        self.shared.vocab_size = self.shared.weight.shape[0]\n","        # retrieve correct absolute scope for embed token wrapper\n","        with tf.compat.v1.variable_scope(\"model.shared\") as shared_abs_scope_name:\n","            pass\n","        # Wraps layer to avoid problems with weight restoring and ensuring we're in the correct TF scope.\n","        embed_tokens = TFWrappedEmbeddings(self.shared, abs_scope_name=shared_abs_scope_name)\n","        self.encoder.set_embed_tokens(embed_tokens)\n","        self.decoder.set_embed_tokens(embed_tokens)\n","\n","    def call(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        decoder_input_ids=None,\n","        decoder_attention_mask=None,\n","        head_mask=None,\n","        decoder_head_mask=None,\n","        cross_attn_head_mask=None,\n","        encoder_outputs= None,\n","        past_key_values=None,\n","        inputs_embeds=None,\n","        decoder_inputs_embeds=None,\n","        use_cache=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","        training=False,\n","        **kwargs\n","    ):\n","\n","        if decoder_input_ids is None and decoder_inputs_embeds is None:\n","            use_cache = False\n","\n","        output_hidden_states = (\n","            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n","        )\n","\n","        if encoder_outputs is None:\n","            encoder_outputs = self.encoder(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                head_mask=head_mask,\n","                inputs_embeds=inputs_embeds,\n","                output_attentions=output_attentions,\n","                output_hidden_states=output_hidden_states,\n","                return_dict=return_dict,\n","                training=training,\n","            )\n","        # If the user passed a tuple for encoder_outputs, we wrap it in a TFBaseModelOutput when return_dict=True\n","        elif return_dict and not isinstance(encoder_outputs):\n","            last_hidden_state=encoder_outputs[0]\n","            hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None\n","            attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None\n","            encoder_outputs = (\n","                last_hidden_state,\n","                hidden_states,\n","                attentions,\n","            )\n","        # If the user passed a TFBaseModelOutput for encoder_outputs, we wrap it in a tuple when return_dict=False\n","        elif not return_dict and not isinstance(encoder_outputs, tuple):\n","            encoder_outputs = encoder_outputs.to_tuple()\n","\n","        decoder_outputs = self.decoder(\n","            decoder_input_ids,\n","            attention_mask=decoder_attention_mask,\n","            encoder_hidden_states=encoder_outputs[0],\n","            encoder_attention_mask=attention_mask,\n","            head_mask=decoder_head_mask,\n","            cross_attn_head_mask=cross_attn_head_mask,\n","            past_key_values=past_key_values,\n","            inputs_embeds=decoder_inputs_embeds,\n","            use_cache=use_cache,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","            training=training,\n","        )\n","\n","        if not return_dict:\n","            return decoder_outputs + encoder_outputs\n","        last_hidden_state=decoder_outputs.last_hidden_state\n","        past_key_values=decoder_outputs.past_key_values\n","        decoder_hidden_states=decoder_outputs.hidden_states\n","        decoder_attentions=decoder_outputs.attentions\n","        cross_attentions=decoder_outputs.cross_attentions\n","        encoder_last_hidden_state=encoder_outputs.last_hidden_state\n","        encoder_hidden_states=encoder_outputs.hidden_states\n","        encoder_attentions=encoder_outputs.attentions\n","        return (\n","            last_hidden_state,\n","            past_key_values,\n","            decoder_hidden_states,\n","            decoder_attentions,\n","            cross_attentions,\n","            encoder_last_hidden_state,\n","            encoder_hidden_states,\n","            encoder_attentions,\n","        )"],"metadata":{"id":"17uVEbgNEOs8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","  def __init__(self, d_model, warmup_steps=4000):\n","    super(CustomSchedule, self).__init__()\n","    \n","    self.d_model = d_model\n","    self.d_model = tf.cast(self.d_model, tf.float32)\n","\n","    self.warmup_steps = warmup_steps\n","    \n","  def __call__(self, step):\n","    arg1 = tf.math.rsqrt(step)\n","    arg2 = step * (self.warmup_steps ** -1.6)\n","    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"],"metadata":{"id":"qDHpnm0Tbys0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["learning_rate = CustomSchedule(1024)\n","optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-8)"],"metadata":{"id":"6vbjP_x7bn_d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TFPegasusModel(tf.keras.Model):\n","    def __init__(self, config, *inputs, **kwargs):\n","        super().__init__(config, *inputs, **kwargs)\n","\n","        self.model = TFPegasusMainLayer(config, name=\"model\")\n","\n","        self.final_logits = tf.keras.layers.Dense(config.vocab_size)\n","\n","        self.config = config\n","\n","    def get_encoder(self):\n","        return self.model.encoder\n","\n","    def get_decoder(self):\n","        return self.model.decoder\n","    \n","    def hf_compute_loss(self, labels, logits):\n","        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n","            from_logits=True, reduction=tf.keras.losses.Reduction.NONE\n","        )\n","        # make sure only labels that are not equal to -100 affect the loss\n","        active_loss = tf.not_equal(tf.reshape(labels, (-1,)), -100)\n","        reduced_logits = tf.boolean_mask(tf.reshape(logits, (-1, shape_list(logits)[2])), active_loss)\n","        labels = tf.boolean_mask(tf.reshape(labels, (-1,)), active_loss)\n","        return loss_fn(labels, reduced_logits)\n","\n","    def call(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        decoder_input_ids=None,\n","        decoder_attention_mask=None,\n","        head_mask=None,\n","        decoder_head_mask=None,\n","        cross_attn_head_mask=None,\n","        encoder_outputs: Optional[Tuple] = None,\n","        past_key_values=None,\n","        inputs_embeds=None,\n","        decoder_inputs_embeds=None,\n","        use_cache=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","        training=False,\n","        **kwargs\n","    ):\n","\n","        outputs = self.model(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            decoder_input_ids=decoder_input_ids,\n","            decoder_attention_mask=decoder_attention_mask,\n","            head_mask=head_mask,\n","            decoder_head_mask=decoder_head_mask,\n","            cross_attn_head_mask=cross_attn_head_mask,\n","            encoder_outputs=encoder_outputs,\n","            past_key_values=past_key_values,\n","            inputs_embeds=inputs_embeds,\n","            decoder_inputs_embeds=decoder_inputs_embeds,\n","            use_cache=use_cache,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","            training=training,\n","        )\n","        output = outputs[0]\n","        return self.final_logits(output)"],"metadata":{"id":"dK2x24d0Gb_G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = TFPegasusModel(config)"],"metadata":{"id":"joCp3MFeiuE5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_input = tokenizer(ipt_data, max_length = 1024, truncation = 'longest_first', padding = 'max_length')\n","_output = tokenizer(opt_data, max_length = 1024, truncation = 'longest_first', padding = 'max_length')"],"metadata":{"id":"zRXrDLD5jIdg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_ids = np.array(_input['input_ids'])\n","input_mask = np.array(_input['attention_mask'])\n","output_ids = np.array(_output['input_ids'])\n","output_mask = np.array(_output['attention_mask'])"],"metadata":{"id":"hTqWpJYXfAMM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BUFFER_SIZE = len(input_ids)\n","BATCH_SIZE = 4\n","dataset_train = tf.data.Dataset.from_tensor_slices((input_ids, input_mask, output_ids, output_mask))\n","dataset_train = dataset_train.cache()\n","dataset_train = dataset_train.shuffle(BUFFER_SIZE)\n","dataset_train = dataset_train.batch(BATCH_SIZE)\n","dataset_train = dataset_train.prefetch(tf.data.experimental.AUTOTUNE)\n","# tf.data.experimental.save(dataset_train, '/content/drive/MyDrive/Khóa luận tốt nghiệp/QA/dataset_train')"],"metadata":{"id":"MYr4mde7jCfq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for x1, x2, x3, x4 in dataset_train:\n","    break"],"metadata":{"id":"gkhRwmPrjjdS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["outputs = model(input_ids=x1,\n","                attention_mask=x2,\n","                decoder_input_ids=x3,\n","                decoder_attention_mask=x4)"],"metadata":{"id":"67VznhhQj1Cf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["outputs.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m13bT43Pk9Xs","executionInfo":{"status":"ok","timestamp":1648455769446,"user_tz":-420,"elapsed":12,"user":{"displayName":"Văn Nghiêm","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03593021723549294443"}},"outputId":"cd3be8c9-219f-45e1-84a3-8616275d6551"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([4, 1024, 48423])"]},"metadata":{},"execution_count":63}]},{"cell_type":"code","source":["def loss_function(real, pred):\n","    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","        from_logits=True, reduction='none')\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = loss_object(real, pred)\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n","\n","train_loss = tf.keras.metrics.Mean()"],"metadata":{"id":"MPajkjAXty2g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@tf.function\n","def train_step(inp, inp_mask, tar, tar_mask):\n","    tar_inp = tar[:, :-1]\n","    tar_real = tar[:, 1:]\n","    tar_mask = tar[:, :-1]\n","    with tf.GradientTape() as tape:\n","        predictions = model(input_ids=inp,\n","                            attention_mask=inp_mask,\n","                            decoder_input_ids=tar_inp,\n","                            decoder_attention_mask=tar_mask)\n","        loss = loss_function(tar_real, predictions)\n","    gradients = tape.gradient(loss, model.trainable_variables) \n","    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","    train_loss(loss)"],"metadata":{"id":"QIr1WGcmlp9z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = 100\n","for epoch in range(epochs):\n","    train_loss.reset_states()\n","    print(\"\\nepoch {}/{}\".format(epoch+1,epochs))\n","    prog = tf.keras.utils.Progbar(len(dataset_train)*BATCH_SIZE, stateful_metrics=['loss'])\n","    for batch, (x1, x2, x3, x4) in enumerate(dataset_train):\n","        train_step(x1, x2, x3, x4)\n","        values=[('loss', train_loss.result().numpy())]\n","        prog.add(BATCH_SIZE, values=values)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rFtj-WvgmuhS","outputId":"96db5009-3126-472b-e704-21116a274fb1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","epoch 1/100\n"," 49368/105420 [=============>................] - ETA: 1:33:53 - loss: 7.7353"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"Eoh8ce7bm8te"},"execution_count":null,"outputs":[]}]}